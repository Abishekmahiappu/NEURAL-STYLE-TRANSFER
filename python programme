{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4617f54e-9b22-41a7-8815-a870c830ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\admin\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torchvision-0.22.1-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.4 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision pillow matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3794fa5f-6b02-468e-85e1-bbf3ee33a2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\content.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mto(device, torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Load content and style images\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m content_img \u001b[38;5;241m=\u001b[39m image_loader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m style_img \u001b[38;5;241m=\u001b[39m image_loader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m content_img\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m style_img\u001b[38;5;241m.\u001b[39msize(), \\\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStyle and content images must be of the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m, in \u001b[0;36mimage_loader\u001b[1;34m(image_name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimage_loader\u001b[39m(image_name):\n\u001b[1;32m---> 23\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_name)\n\u001b[0;32m     24\u001b[0m     image \u001b[38;5;241m=\u001b[39m loader(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mto(device, torch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\content.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# Load device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image size\n",
    "imsize = 512 if torch.cuda.is_available() else 128\n",
    "\n",
    "# Image loader\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize((imsize, imsize)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Function to load and transform image\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "# Load content and style images\n",
    "content_img = image_loader(\"content.jpg\")\n",
    "style_img = image_loader(\"style.jpg\")\n",
    "\n",
    "assert content_img.size() == style_img.size(), \\\n",
    "    \"Style and content images must be of the same size\"\n",
    "\n",
    "# Display function\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Loss Functions\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.loss = nn.functional.mse_loss(x, self.target)\n",
    "        return x\n",
    "\n",
    "def gram_matrix(input):\n",
    "    batch_size, feature_maps, h, w = input.size()\n",
    "    features = input.view(batch_size * feature_maps, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(batch_size * feature_maps * h * w)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        G = gram_matrix(x)\n",
    "        self.loss = nn.functional.mse_loss(G, self.target)\n",
    "        return x\n",
    "\n",
    "# Load VGG19 model\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# Normalization\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = mean.clone().detach().view(-1, 1, 1)\n",
    "        self.std = std.clone().detach().view(-1, 1, 1)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "# Define model layers\n",
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "# Build model\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                                style_img, content_img):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "    \n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    \n",
    "    model = nn.Sequential(normalization)\n",
    "    i = 0\n",
    "\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f'content_loss_{i}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f'style_loss_{i}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Trim off after last content/style loss\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "    model = model[:i + 1]\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "# Input image\n",
    "input_img = content_img.clone()\n",
    "plt.figure()\n",
    "imshow(input_img, title='Input Image')\n",
    "\n",
    "# Optimization\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1000000, content_weight=1):\n",
    "    \n",
    "    model, style_losses, content_losses = get_style_model_and_losses(\n",
    "        cnn, normalization_mean, normalization_std, style_img, content_img\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "\n",
    "    print(\"Optimizing...\")\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            loss = style_score * style_weight + content_score * content_weight\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f\"Step {run[0]}:\")\n",
    "                print(f\"Style Loss: {style_score.item():4f} Content Loss: {content_score.item():4f}\")\n",
    "\n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img\n",
    "\n",
    "# Run the model\n",
    "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                            content_img, style_img, input_img)\n",
    "\n",
    "# Display result\n",
    "plt.figure()\n",
    "imshow(output, title='Output Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fdcee-4dbd-492b-8971-bd41911a1607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
